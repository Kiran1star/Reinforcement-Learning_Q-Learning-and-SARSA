{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_assignment1_checkpoint.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YV2irfUSFEhW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BkqobPPBmSn"
      },
      "outputs": [],
      "source": [
        "# \"I certify that the code and data in this assignment were generated independently, using only the tools and resources defined in the course and that I did not receive any external help, coaching or contributions during the production of this work.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogD9CuynB2id",
        "outputId": "0e69df3b-426f-4aab-85a4-e91ec6784efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from gym import spaces"
      ],
      "metadata": {
        "id": "5fVw3MKhB4T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import widgets\n",
        "import time"
      ],
      "metadata": {
        "id": "fy8IlAbrB6k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part I"
      ],
      "metadata": {
        "id": "YV2irfUSFEhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridEnvironment(gym.Env):\n",
        "    metadata = { 'render.modes': [] }\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.observation_space = spaces.Discrete(16) #Intializing a 4x4 grid with 16 states: {s1, s2, s3,....s16}\n",
        "        self.action_space = spaces.Discrete(4) #Intializing 4 actions:  {0: down, 1: up, 2: right, 3: left}\n",
        "        self.done = False\n",
        "        #rewards = {-2, -1, 0, 3, 4, 100} defined below\n",
        "        \n",
        "    def reset(self):\n",
        "  \n",
        "        self.agent_pos = [0, 0] #start position\n",
        "        self.goal_pos = [3, 3] #target position (+100)\n",
        "        self.danger1_pos = [1,1] #first danger position (-1)\n",
        "        self.danger2_pos = [2,2] #second danger position (-2)\n",
        "        self.gold1_pos = [2,0] #First positive reward position (+3)\n",
        "        self.gold2_pos = [3,0] #Second positive reward position (+4)\n",
        "        self.done = False\n",
        "        self.state = np.zeros((4,4))\n",
        "        observation = self.state.flatten()\n",
        "        return observation\n",
        "    \n",
        "    def step(self, action, deterministic = False, stochastic = False):\n",
        "\n",
        "        #determinstic environment\n",
        "        if deterministic == True:\n",
        "          epsilon1, epsilon2 = 1, 1 #With probability 1 Agent chooses given action\n",
        "\n",
        "        #stochastic environment\n",
        "        if stochastic == True:\n",
        "          epsilon1, epsilon2 = 0.7, 0.8 #Transistion probabilites of 0.7 and 0.8 \n",
        "          #Σ(p(s', r/s, down) = 0.7 + 0.3 = 1\n",
        "          #Σ(p(s', r/s, right) = 0.8 + 0.2 = 1 \n",
        "\n",
        "        #Actions\n",
        "        if action == 0: #down\n",
        "            rand_num1 = np.random.random()\n",
        "            if epsilon1 >= rand_num1:  \n",
        "              self.agent_pos[0] += 1   \n",
        "            else: #For all states in stochastic environment when Down action is choosen: Agent chooses down state with a transition probability of 0.7 and up state with 0.3 transition probaility\n",
        "              self.agent_pos[0] -= 1\n",
        "              # print(\"Up state is choosen with 0.3 probability instead of Down\")\n",
        "\n",
        "        if action == 1: #up\n",
        "            self.agent_pos[0] -= 1\n",
        "\n",
        "        if action == 2: #right\n",
        "            rand_num2 = np.random.random()\n",
        "            if epsilon2 >= rand_num2:  \n",
        "              self.agent_pos[1] += 1        \n",
        "            else:#For all states in stochastic environment when Right action is choosen: Agent chooses right state with a transition probability of 0.8 and left state with 0.2 transition probaility\n",
        "              self.agent_pos[1] -= 1\n",
        "              # print(\"Left state is choosen with 0.2 probability instead of Right\")\n",
        "\n",
        "        if action == 3: #left\n",
        "            self.agent_pos[1] -= 1\n",
        "          \n",
        "        self.agent_pos = np.clip(self.agent_pos, 0, 3) #ensuring agent doesn't go out of the grid #One of the ways to ensure safety in the environment\n",
        "        self.state = np.zeros((4,4))\n",
        "\n",
        "        observation = self.state.flatten()\n",
        "        \n",
        "        reward = 0 #Intializing reward to zero\n",
        "        if (self.agent_pos == self.goal_pos).all():\n",
        "            reward = 100 #A reward of 5 if it reaches target position\n",
        "            self.done = True\n",
        "\n",
        "        #Rewards structure\n",
        "        elif (self.agent_pos == self.danger1_pos).all():\n",
        "            reward = -1 #A negative reward -1 if it enters 1st danger position\n",
        "\n",
        "        elif (self.agent_pos == self.danger2_pos).all():\n",
        "            reward = -2 #A negative reward of -2 if it enters 2nd danger position\n",
        "\n",
        "        elif (self.agent_pos == self.gold1_pos).all():\n",
        "            reward = 3 #A reward of +3 at [2,0]\n",
        "\n",
        "        elif (self.agent_pos == self.gold2_pos).all():\n",
        "            reward = 4 #A reward of +3 at [3,0]\n",
        "\n",
        "        return reward, self.agent_pos, self.done\n",
        "        \n",
        "    def render(self):\n",
        "        plt.imshow(self.state)"
      ],
      "metadata": {
        "id": "wswAhd84B9NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAgent: #Definig the Random agent class\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.observation_space = env.observation_space\n",
        "        self.action_space = env.action_space\n",
        "\n",
        "    def step(self, observation): #Random agent chooses random action every time we call step method\n",
        "        return np.random.choice(self.action_space.n)"
      ],
      "metadata": {
        "id": "Lvrb9DK8CKTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deterministic environment\n",
        "env = GridEnvironment()\n",
        "agent = RandomAgent(env)#creating a random agent to explore the given environments \n",
        "obs = env.reset()#resets the environment to its initial configuration\n",
        "\n",
        "done = False\n",
        "timestep = 0\n",
        "epsilon = 0.8 # For all states in deterministic environment p(s', r/s, a) = {0, 1}: Either action taken or No action taken\n",
        "while timestep < 30:#running for 30 timesteps\n",
        "\n",
        "    action = agent.step(obs)\n",
        "    timestep += 1\n",
        "\n",
        "    # deterministic\n",
        "    rand_num = np.random.random()\n",
        "    if epsilon > rand_num: #Choosing an action in deterministic environment\n",
        "      reward, pos, done = env.step(action, deterministic = True)#calling step method to randomly choose an action\n",
        "      print(\"Timestep: {}, Reward: {}\".format(timestep, reward))\n",
        "    else:\n",
        "      print(\"Timestep: {}, No action taken\".format(timestep)) #Random agent doesn't choose any action in deterministic environment\n",
        "    \n",
        "    if done == True:\n",
        "      print(\"Target Reached!!\") #End the program if Random Agent reaches the target position\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKETSCEICM8J",
        "outputId": "4dc33358-2edd-4899-8f16-6905ef533866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestep: 1, Reward: 0\n",
            "Timestep: 2, Reward: 3\n",
            "Timestep: 3, No action taken\n",
            "Timestep: 4, Reward: 0\n",
            "Timestep: 5, Reward: 3\n",
            "Timestep: 6, No action taken\n",
            "Timestep: 7, No action taken\n",
            "Timestep: 8, Reward: 3\n",
            "Timestep: 9, Reward: 4\n",
            "Timestep: 10, Reward: 0\n",
            "Timestep: 11, Reward: 0\n",
            "Timestep: 12, Reward: 0\n",
            "Timestep: 13, Reward: 0\n",
            "Timestep: 14, Reward: -1\n",
            "Timestep: 15, No action taken\n",
            "Timestep: 16, No action taken\n",
            "Timestep: 17, Reward: 0\n",
            "Timestep: 18, Reward: 3\n",
            "Timestep: 19, Reward: 0\n",
            "Timestep: 20, Reward: -2\n",
            "Timestep: 21, Reward: 0\n",
            "Timestep: 22, Reward: 100\n",
            "Target Reached!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stochastic environment\n",
        "env = GridEnvironment()\n",
        "agent = RandomAgent(env)#creating a random agent to explore the given environments \n",
        "obs = env.reset()#resets the environment to its initial configuration\n",
        "\n",
        "\n",
        "timestep = 0\n",
        "while timestep < 30: #running for 30 timesteps\n",
        "\n",
        "    action = agent.step(obs)#calling step method to randomly choose an action\n",
        "    timestep += 1\n",
        "\n",
        "    #stochastic\n",
        "    reward, pos, done = env.step(action, stochastic = True)#Choosing an action in stochastic environment\n",
        "    print(\"Timestep: {}, Reward: {}\".format(timestep, reward)) #Tracks the timestep and reward after every episode\n",
        "\n",
        "    if done == True:\n",
        "      print(\"Target Reached!!\") #End the program if Random Agent reaches the target position\n",
        "      break"
      ],
      "metadata": {
        "id": "ZB7wdVkdCO49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca53e2f-2816-45ae-aa9c-8d0cab36864d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestep: 1, Reward: 0\n",
            "Timestep: 2, Reward: 0\n",
            "Timestep: 3, Reward: 0\n",
            "Timestep: 4, Reward: 0\n",
            "Timestep: 5, Reward: 0\n",
            "Timestep: 6, Reward: 0\n",
            "Timestep: 7, Reward: 0\n",
            "Timestep: 8, Reward: 0\n",
            "Timestep: 9, Reward: 0\n",
            "Timestep: 10, Reward: 0\n",
            "Timestep: 11, Reward: 0\n",
            "Timestep: 12, Reward: 0\n",
            "Timestep: 13, Reward: 0\n",
            "Timestep: 14, Reward: 0\n",
            "Timestep: 15, Reward: 0\n",
            "Timestep: 16, Reward: -1\n",
            "Timestep: 17, Reward: 0\n",
            "Timestep: 18, Reward: -1\n",
            "Timestep: 19, Reward: 0\n",
            "Timestep: 20, Reward: 3\n",
            "Timestep: 21, Reward: 4\n",
            "Timestep: 22, Reward: 0\n",
            "Timestep: 23, Reward: 0\n",
            "Timestep: 24, Reward: 0\n",
            "Timestep: 25, Reward: 100\n",
            "Target Reached!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8AdfV8aowka0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}